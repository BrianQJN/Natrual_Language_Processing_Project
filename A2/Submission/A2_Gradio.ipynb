{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_vectors, k1=2, k2=4, n1=4, n2=16):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vectors)\n",
    "        embedding_dim = embedding_vectors.size(1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        # (in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, n1, (k1, embedding_dim), bias=False)\n",
    "        self.conv2 = nn.Conv2d(1, n2, (k2, embedding_dim), bias=False)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(n1 + n2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Add an extra dimension for the channel\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Apply Convolutional layers and ReLU activation\n",
    "        x1 = F.relu(self.conv1(x)).squeeze(3)\n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)\n",
    "\n",
    "        # Max pooling over the sentence length\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x).squeeze(1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, embedding_vectors):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vectors)\n",
    "        self.fc = nn.Linear(embedding_vectors.size(1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=0)\n",
    "        x = self.fc(x).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BaselineModel(\n",
       "   (embedding): Embedding(400000, 100)\n",
       "   (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       " ),\n",
       " CNNClassifier(\n",
       "   (embedding): Embedding(400000, 100)\n",
       "   (conv1): Conv2d(1, 4, kernel_size=(2, 100), stride=(1, 1), bias=False)\n",
       "   (conv2): Conv2d(1, 16, kernel_size=(4, 100), stride=(1, 1), bias=False)\n",
       "   (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " <torchtext.vocab.vectors.GloVe at 0x2bdfcb640>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GloVe embeddings\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_baseline = BaselineModel(glove.vectors).to(device)\n",
    "model_baseline.load_state_dict(torch.load(\"model_baseline.pt\"))\n",
    "\n",
    "model_CNN = CNNClassifier(glove.vectors, k1=2, n1=4, k2=4, n2=16).to(device)\n",
    "model_CNN.load_state_dict(torch.load(\"model_CNN.pt\"))\n",
    "\n",
    "model_baseline.eval()  # setting the model to evaluation mode\n",
    "model_CNN.eval()  # setting the model to evaluation mode\n",
    "\n",
    "model_baseline, model_CNN, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(sentence):\n",
    "    tokens = sentence.split()\n",
    "    token_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
    "    token_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
    "    \n",
    "    return token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model_choice):\n",
    "    \"\"\"\n",
    "    Predict the classification of the sentence using the selected model.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The input sentence for prediction.\n",
    "    - model_choice (str): The selected model (\"Baseline\" or \"CNN\").\n",
    "\n",
    "    Returns:\n",
    "    - Prediction result as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the input sentence to get the tensor\n",
    "    token_tensor = process_input(sentence)\n",
    "    \n",
    "    if model_choice == \"Baseline\":\n",
    "        # Get predictions from the baseline model\n",
    "        output = model_baseline(token_tensor)\n",
    "        print(output)\n",
    "    else:\n",
    "        # Get predictions from the CNN model\n",
    "        output = model_CNN(token_tensor)\n",
    "        \n",
    "    prob = torch.sigmoid(output)  # Using sigmoid activation instead of softmax\n",
    "    predicted_class = 'subjective' if prob[0] > 0.5 else 'objective'\n",
    "    \n",
    "    return f\"Class: {predicted_class}, Probability: {prob[0].item():.4f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3759], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Class: objective, Probability: 0.0331'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"his life is a whirl of seedy bars , seedier girlfriends and a wife two towns away .\", \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5610], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-3.6983], grad_fn=<SqueezeBackward1>)\n",
      "tensor([3.5610], grad_fn=<SqueezeBackward1>)\n",
      "tensor([3.5610], grad_fn=<SqueezeBackward1>)\n",
      "tensor([3.6626], grad_fn=<SqueezeBackward1>)\n",
      "tensor([6.8853], grad_fn=<SqueezeBackward1>)\n",
      "tensor([1.8826], grad_fn=<SqueezeBackward1>)\n",
      "tensor([1.8826], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-1.5518], grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.5581], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-1.5518], grad_fn=<SqueezeBackward1>)\n",
      "tensor([-4.8716], grad_fn=<SqueezeBackward1>)\n",
      "tensor([7.9978], grad_fn=<SqueezeBackward1>)\n",
      "tensor([7.5710], grad_fn=<SqueezeBackward1>)\n",
      "tensor([7.5710], grad_fn=<SqueezeBackward1>)\n",
      "tensor([6.6265], grad_fn=<SqueezeBackward1>)\n",
      "tensor([1.2875], grad_fn=<SqueezeBackward1>)\n",
      "tensor([1.2875], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gApp = gr.Interface(fn=predict, \n",
    "                        inputs=[gr.Textbox(label=\"type a sentence here\"), \n",
    "                                gr.Radio([\"Baseline\", \"CNN\"], label=\"Model Choice\")], \n",
    "                        outputs=gr.Textbox(label=\"Model's Prediction\"))\n",
    "\n",
    "gApp.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run each cell one by one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
