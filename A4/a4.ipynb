{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# sample up to 30 tokens\n",
    "torch.manual_seed(0)\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2, Top p: 0.1\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2, Top p: 0.3\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2, Top p: 0.5\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2, Top p: 0.7\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.2, Top p: 0.9\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important part of the global economy.\\n\\n\"The world']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.5, Top p: 0.1\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.5, Top p: 0.3\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.5, Top p: 0.5\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important part of the global economy.\\n\\n\"The world']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.5, Top p: 0.7\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important part of the global economy.\\n\\n\"The world']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.5, Top p: 0.9\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important source of energy. It is also a very important source']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7, Top p: 0.1\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7, Top p: 0.3\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7, Top p: 0.5\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important part of the global economy. It is also important to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7, Top p: 0.7\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important source of energy. It is also a very important source']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7, Top p: 0.9\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very big problem in the world today,\" he said.\\n\\n']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0, Top p: 0.1\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0, Top p: 0.3\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important part of the global economy. It is also important for']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0, Top p: 0.5\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because it is a very important source of greenhouse gas emissions,\" he said.\\n\\n']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0, Top p: 0.7\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because this is the only way to tackle climate change,\" he said.\\n\\n\"']\n",
      "\n",
      "\n",
      "Temperature: 1.0, Top p: 0.9\n",
      "Generated Text: ['It is important for all countries to try harder to reduce carbon emissions because countries are now more comfortable with the impact of global warming on the environment,\" the']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_sequence = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "\n",
    "temperature_values = [0.2, 0.5, 0.7, 1.0]\n",
    "top_p_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for temperature in temperature_values:\n",
    "    for top_p in top_p_values:\n",
    "        input_ids = tokenizer(input_sequence, return_tensors=\"pt\").input_ids\n",
    "        torch.manual_seed(0)\n",
    "        outputs = model.generate(input_ids, do_sample=True, max_length=30, temperature=temperature, top_p=top_p)\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        print(f\"Temperature: {temperature}, Top p: {top_p}\")\n",
    "        print(\"Generated Text:\", generated_text)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it</td>\n",
       "      <td>0.205233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that</td>\n",
       "      <td>0.196039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>0.153607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more</td>\n",
       "      <td>0.075481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>likely</td>\n",
       "      <td>0.268765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>0.587506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>0.302946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fact</td>\n",
       "      <td>0.124495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>0.723886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>climate</td>\n",
       "      <td>0.364284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>warming</td>\n",
       "      <td>0.949618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,\"</td>\n",
       "      <td>0.220035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>their</td>\n",
       "      <td>0.605831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>environment</td>\n",
       "      <td>0.360961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,\"</td>\n",
       "      <td>0.298489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>said</td>\n",
       "      <td>0.352416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token  Probability\n",
       "0             it     0.205233\n",
       "1           that     0.196039\n",
       "2            not     0.153607\n",
       "3           more     0.075481\n",
       "4         likely     0.268765\n",
       "5           with     0.587506\n",
       "6            the     0.302946\n",
       "7           fact     0.124495\n",
       "8             of     0.723886\n",
       "9        climate     0.364284\n",
       "10       warming     0.949618\n",
       "11            ,\"     0.220035\n",
       "12         their     0.605831\n",
       "13   environment     0.360961\n",
       "14            ,\"     0.298489\n",
       "15          said     0.352416"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the tokenizer and model from the HuggingFace Transformers library.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the input sequence.\n",
    "input_sequence = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "\n",
    "# Encode the input text.\n",
    "input_ids = tokenizer(input_sequence, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Set a seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Generate text using the model.\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "# Decode the generated text.\n",
    "generated_text = tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)\n",
    "\n",
    "# Retrieve the scores (logits) for each token generated.\n",
    "generated_scores = outputs['scores']\n",
    "\n",
    "generated_tokens = []\n",
    "probabilities = []\n",
    "\n",
    "for i, score in enumerate(generated_scores):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probs = torch.softmax(score, dim=-1)\n",
    "    # Get the index of the maximum probability token for each step\n",
    "    max_prob_index = probs.argmax(-1).item()\n",
    "    # Get the actual token using the tokenizer\n",
    "    token = tokenizer.decode(max_prob_index)\n",
    "    # Get the corresponding max probability\n",
    "    max_prob = probs[0, max_prob_index].item()\n",
    "    # Append the token and its probability to the lists\n",
    "    generated_tokens.append(token)\n",
    "    probabilities.append(max_prob)\n",
    "\n",
    "# Create a DataFrame with the generated text and their probabilities\n",
    "df = pd.DataFrame({\n",
    "    'Token': generated_tokens,\n",
    "    'Probability': probabilities\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2.cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define arrays for different temperature and top_p values you want to test.\n",
    "temperature_values = [0.2, 0.5, 0.7, 1.0]\n",
    "top_p_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Dictionary to store DataFrames for each combination of temperature and top_p\n",
    "dfs = {}\n",
    "\n",
    "for temperature, top_p in product(temperature_values, top_p_values):\n",
    "    # Generate text using the model with the current combination of temperature and top_p\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=30,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)\n",
    "\n",
    "    # Retrieve the scores (logits) for each token generated\n",
    "    generated_scores = outputs['scores']\n",
    "\n",
    "    generated_tokens = []\n",
    "    probabilities = []\n",
    "\n",
    "    for i, score in enumerate(generated_scores):\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probs = torch.softmax(score, dim=-1)\n",
    "        # Get the index of the maximum probability token for each step\n",
    "        max_prob_index = probs.argmax(-1).item()\n",
    "        # Get the actual token using the tokenizer\n",
    "        token = tokenizer.decode(max_prob_index)\n",
    "        # Get the corresponding max probability\n",
    "        max_prob = probs[0, max_prob_index].item()\n",
    "        # Append the token and its probability to the lists\n",
    "        generated_tokens.append(token)\n",
    "        probabilities.append(max_prob)\n",
    "\n",
    "    # Create a DataFrame with the generated text and their probabilities\n",
    "    df = pd.DataFrame({\n",
    "        'Token': generated_tokens,\n",
    "        'Probability': probabilities\n",
    "    })\n",
    "\n",
    "    # Store the DataFrame in the dictionary using a tuple of (temperature, top_p) as the key\n",
    "    dfs[(temperature, top_p)] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.1)\n",
      "       Token  Probability\n",
      "0         it          1.0\n",
      "1         is          1.0\n",
      "2          a          1.0\n",
      "3        key          1.0\n",
      "4       part          1.0\n",
      "5         of          1.0\n",
      "6        the          1.0\n",
      "7     global          1.0\n",
      "8    economy          1.0\n",
      "9          .          1.0\n",
      "10        \\n          1.0\n",
      "11        \\n          1.0\n",
      "12         \"          1.0\n",
      "13        We          1.0\n",
      "14      need          1.0\n",
      "15        to          1.0\n",
      "\n",
      "(0.2, 0.3)\n",
      "       Token  Probability\n",
      "0         it          1.0\n",
      "1         is          1.0\n",
      "2          a          1.0\n",
      "3        key          1.0\n",
      "4       part          1.0\n",
      "5         of          1.0\n",
      "6        the          1.0\n",
      "7     global          1.0\n",
      "8    economy          1.0\n",
      "9          .          1.0\n",
      "10        \\n          1.0\n",
      "11        \\n          1.0\n",
      "12         \"          1.0\n",
      "13        We          1.0\n",
      "14      need          1.0\n",
      "15        to          1.0\n",
      "\n",
      "(0.2, 0.5)\n",
      "       Token  Probability\n",
      "0         it     1.000000\n",
      "1         is     1.000000\n",
      "2          a     1.000000\n",
      "3        key     0.569538\n",
      "4       part     1.000000\n",
      "5         of     1.000000\n",
      "6        the     1.000000\n",
      "7     global     1.000000\n",
      "8    economy     1.000000\n",
      "9          .     1.000000\n",
      "10        \\n     1.000000\n",
      "11        \\n     1.000000\n",
      "12         \"     1.000000\n",
      "13        We     1.000000\n",
      "14      need     1.000000\n",
      "15        to     1.000000\n",
      "\n",
      "(0.2, 0.7)\n",
      "           Token  Probability\n",
      "0             it     1.000000\n",
      "1             is     1.000000\n",
      "2              a     1.000000\n",
      "3            key     0.569538\n",
      "4    contributor     0.529172\n",
      "5             to     1.000000\n",
      "6        climate     1.000000\n",
      "7         change     1.000000\n",
      "8              .     0.692986\n",
      "9             \\n     1.000000\n",
      "10            \\n     1.000000\n",
      "11             \"     1.000000\n",
      "12            We     0.672990\n",
      "13         world     1.000000\n",
      "14         needs     1.000000\n",
      "15            to     1.000000\n",
      "\n",
      "(0.2, 0.9)\n",
      "       Token  Probability\n",
      "0         it     1.000000\n",
      "1         is     1.000000\n",
      "2          a     0.858110\n",
      "3        key     0.414162\n",
      "4       part     1.000000\n",
      "5         of     1.000000\n",
      "6        the     1.000000\n",
      "7     global     0.878255\n",
      "8    economy     1.000000\n",
      "9          .     0.863618\n",
      "10        \\n     1.000000\n",
      "11        \\n     1.000000\n",
      "12         \"     1.000000\n",
      "13        We     0.722209\n",
      "14      need     1.000000\n",
      "15        to     1.000000\n",
      "\n",
      "(0.5, 0.1)\n",
      "       Token  Probability\n",
      "0         it          1.0\n",
      "1         is          1.0\n",
      "2          a          1.0\n",
      "3        key          1.0\n",
      "4       part          1.0\n",
      "5         of          1.0\n",
      "6        the          1.0\n",
      "7     global          1.0\n",
      "8    economy          1.0\n",
      "9          .          1.0\n",
      "10        \\n          1.0\n",
      "11        \\n          1.0\n",
      "12         \"          1.0\n",
      "13        We          1.0\n",
      "14      need          1.0\n",
      "15        to          1.0\n",
      "\n",
      "(0.5, 0.3)\n",
      "           Token  Probability\n",
      "0             it     1.000000\n",
      "1             is     1.000000\n",
      "2              a     1.000000\n",
      "3            key     0.527971\n",
      "4    contributor     1.000000\n",
      "5             to     1.000000\n",
      "6        climate     1.000000\n",
      "7         change     1.000000\n",
      "8              .     1.000000\n",
      "9             \\n     1.000000\n",
      "10            \\n     1.000000\n",
      "11             \"     1.000000\n",
      "12            We     1.000000\n",
      "13          need     1.000000\n",
      "14            to     1.000000\n",
      "15          take     0.522332\n",
      "\n",
      "(0.5, 0.5)\n",
      "       Token  Probability\n",
      "0         it     0.783926\n",
      "1         is     1.000000\n",
      "2          a     1.000000\n",
      "3        key     0.364930\n",
      "4       part     0.823235\n",
      "5         of     1.000000\n",
      "6        the     1.000000\n",
      "7     global     0.687915\n",
      "8    economy     1.000000\n",
      "9          .     1.000000\n",
      "10        \\n     1.000000\n",
      "11        \\n     1.000000\n",
      "12         \"     1.000000\n",
      "13        We     0.594397\n",
      "14     world     0.799684\n",
      "15    States     1.000000\n",
      "\n",
      "(0.5, 0.7)\n",
      "       Token  Probability\n",
      "0         it     0.660738\n",
      "1        are     1.000000\n",
      "2        the     0.681424\n",
      "3      going     0.493456\n",
      "4         to     1.000000\n",
      "5         be     1.000000\n",
      "6       able     1.000000\n",
      "7         to     1.000000\n",
      "8       meet     0.439359\n",
      "9      their     1.000000\n",
      "10   targets     0.670539\n",
      "11        ,\"     0.606320\n",
      "12        \\n     1.000000\n",
      "13        \\n     1.000000\n",
      "14         \"     1.000000\n",
      "15        We     0.535048\n",
      "\n",
      "(0.5, 0.9)\n",
      "       Token  Probability\n",
      "0         it     0.530216\n",
      "1         is     0.722524\n",
      "2          a     0.576105\n",
      "3        key     0.290617\n",
      "4       step     0.397387\n",
      "5    forward     0.835165\n",
      "6        for     0.344547\n",
      "7         \\n     0.548516\n",
      "8         \\n     1.000000\n",
      "9          \"     1.000000\n",
      "10        We     0.447463\n",
      "11     world     0.526535\n",
      "12        is     0.519003\n",
      "13     going     0.170117\n",
      "14         a     1.000000\n",
      "15   serious     0.179456\n",
      "\n",
      "(0.7, 0.1)\n",
      "       Token  Probability\n",
      "0         it          1.0\n",
      "1         is          1.0\n",
      "2          a          1.0\n",
      "3        key          1.0\n",
      "4       part          1.0\n",
      "5         of          1.0\n",
      "6        the          1.0\n",
      "7     global          1.0\n",
      "8    economy          1.0\n",
      "9          .          1.0\n",
      "10        \\n          1.0\n",
      "11        \\n          1.0\n",
      "12         \"          1.0\n",
      "13        We          1.0\n",
      "14      need          1.0\n",
      "15        to          1.0\n",
      "\n",
      "(0.7, 0.3)\n",
      "       Token  Probability\n",
      "0         it     1.000000\n",
      "1         is     1.000000\n",
      "2          a     1.000000\n",
      "3        key     0.519990\n",
      "4       part     1.000000\n",
      "5         of     1.000000\n",
      "6        the     1.000000\n",
      "7     global     0.637507\n",
      "8    economy     1.000000\n",
      "9          .     1.000000\n",
      "10        \\n     1.000000\n",
      "11        \\n     1.000000\n",
      "12         \"     1.000000\n",
      "13        We     0.567824\n",
      "14     world     0.728851\n",
      "15     needs     1.000000\n",
      "\n",
      "(0.7, 0.5)\n",
      "         Token  Probability\n",
      "0           it     0.569215\n",
      "1           is     1.000000\n",
      "2            a     0.625792\n",
      "3          key     0.355786\n",
      "4    important     1.000000\n",
      "5         step     0.536085\n",
      "6           in     0.737721\n",
      "7          the     1.000000\n",
      "8        right     0.569744\n",
      "9           of     1.000000\n",
      "10    reducing     1.000000\n",
      "11      global     0.410551\n",
      "12         gas     1.000000\n",
      "13   emissions     1.000000\n",
      "14          ,\"     1.000000\n",
      "15        said     0.561985\n",
      "\n",
      "(0.7, 0.7)\n",
      "        Token  Probability\n",
      "0          it     0.424690\n",
      "1       world     0.476892\n",
      "2          is     0.601853\n",
      "3    changing     0.386280\n",
      "4       doing     0.151408\n",
      "5           a     0.881852\n",
      "6     serious     0.287765\n",
      "7     warming     0.462886\n",
      "8      crisis     1.000000\n",
      "9           .     0.514437\n",
      "10       said     0.532727\n",
      "11        the     0.306551\n",
      "12       Mann     1.000000\n",
      "13          ,     1.000000\n",
      "14   director     0.385599\n",
      "15         of     1.000000\n",
      "\n",
      "(0.7, 0.9)\n",
      "         Token  Probability\n",
      "0           it     0.364417\n",
      "1          are     0.576419\n",
      "2          the     0.356886\n",
      "3          for     1.000000\n",
      "4          the     0.269561\n",
      "5          the     0.425486\n",
      "6           of     0.448012\n",
      "7        about     0.212487\n",
      "8         half     0.138672\n",
      "9            .     0.381346\n",
      "10           3     1.000000\n",
      "11     percent     0.475555\n",
      "12          of     0.321155\n",
      "13       their     0.350086\n",
      "14   emissions     0.400688\n",
      "15          by     0.376395\n",
      "\n",
      "(1.0, 0.1)\n",
      "       Token  Probability\n",
      "0         it          1.0\n",
      "1         is          1.0\n",
      "2          a          1.0\n",
      "3        key          1.0\n",
      "4       part          1.0\n",
      "5         of          1.0\n",
      "6        the          1.0\n",
      "7     global          1.0\n",
      "8    economy          1.0\n",
      "9          .          1.0\n",
      "10        \\n          1.0\n",
      "11        \\n          1.0\n",
      "12         \"          1.0\n",
      "13        We          1.0\n",
      "14      need          1.0\n",
      "15        to          1.0\n",
      "\n",
      "(1.0, 0.3)\n",
      "          Token  Probability\n",
      "0            it     0.655736\n",
      "1         world     0.536339\n",
      "2            is     1.000000\n",
      "3      changing     0.475628\n",
      "4        faster     0.575176\n",
      "5            we     0.526485\n",
      "6           are     1.000000\n",
      "7           not     0.252075\n",
      "8          more     0.536446\n",
      "9           and     0.504282\n",
      "10      dioxide     1.000000\n",
      "11         from     0.593703\n",
      "12          the     1.000000\n",
      "13   atmosphere     1.000000\n",
      "14            .     0.524484\n",
      "15           we     1.000000\n",
      "\n",
      "(1.0, 0.5)\n",
      "      Token  Probability\n",
      "0        it     0.400365\n",
      "1       are     0.584360\n",
      "2       the     0.410531\n",
      "3        do     0.238375\n",
      "4       and     0.333780\n",
      "5         a     0.305931\n",
      "6      have     0.467331\n",
      "7      seen     0.812432\n",
      "8         ,     1.000000\n",
      "9    reduce     0.181713\n",
      "10       to     0.698795\n",
      "11     very     0.223964\n",
      "12      tax     0.372545\n",
      "13        .     0.405893\n",
      "14       \\n     1.000000\n",
      "15       \\n     1.000000\n",
      "\n",
      "(1.0, 0.7)\n",
      "          Token  Probability\n",
      "0            it     0.292384\n",
      "1            is     0.628325\n",
      "2            no     0.401125\n",
      "3    substitute     0.115966\n",
      "4          that     1.000000\n",
      "5           the     0.270186\n",
      "6            we     0.636145\n",
      "7            do     0.506275\n",
      "8            to     0.518522\n",
      "9       achieve     0.250444\n",
      "10        their     0.790737\n",
      "11    emissions     0.556054\n",
      "12            ,     0.393965\n",
      "13         they     0.630283\n",
      "14         will     0.491681\n",
      "15         have     0.506948\n",
      "\n",
      "(1.0, 0.9)\n",
      "      Token  Probability\n",
      "0        it     0.226955\n",
      "1    change     1.000000\n",
      "2        is     0.506770\n",
      "3         a     0.517151\n",
      "4     risks     0.437757\n",
      "5        to     0.714017\n",
      "6        is     0.125475\n",
      "7        to     0.631149\n",
      "8       are     0.199316\n",
      "9    cannot     0.262451\n",
      "10       to     1.000000\n",
      "11     take     0.332391\n",
      "12      now     0.360882\n",
      "13       ,\"     0.306363\n",
      "14       \\n     0.893405\n",
      "15       \\n     1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    print(df)\n",
    "    print(dfs[df])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from treelib import Node, Tree\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# Initialize the tokenizer and model from the HuggingFace Transformers library.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the input sequence.\n",
    "input_sequence = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "\n",
    "# Encode the input text.\n",
    "input_ids = tokenizer(input_sequence, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Define arrays for different temperature and top_p values you want to test.\n",
    "temperature_values = [0.2, 0.5, 0.7, 1.0]\n",
    "top_p_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Dictionary to store trees for each combination of temperature and top_p\n",
    "trees = {}\n",
    "\n",
    "# Loop over each combination of temperature and top_p values\n",
    "for temperature, top_p in product(temperature_values, top_p_values):\n",
    "    # Generate text using the model with the current combination of temperature and top_p\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=30,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Retrieve the scores (logits) for each token generated\n",
    "    generated_scores = outputs['scores']\n",
    "\n",
    "    # Create a new tree for this combination\n",
    "    tree = Tree()\n",
    "    tree.create_node(f\"Root (Temp: {temperature}, Top-p: {top_p})\", \"root\")  # Root node with params\n",
    "\n",
    "    for i, score in enumerate(generated_scores):\n",
    "        parent_id = f\"step_{i-1}_option_0\" if i > 0 else \"root\"\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probs = torch.softmax(score, dim=-1)\n",
    "\n",
    "        # Get the top 3 probabilities and their token indices\n",
    "        top_probs, top_indices = torch.topk(probs, 3, dim=-1)\n",
    "\n",
    "        for j in range(top_indices.size(1)):\n",
    "            token_id = top_indices[0][j].item()\n",
    "            prob = top_probs[0][j].item()\n",
    "            token = tokenizer.decode([token_id])  # Decode the list of one token ID\n",
    "\n",
    "            node_id = f\"step_{i}_option_{j}\"\n",
    "            node_label = f\"{token} ({prob:.2f})\"\n",
    "            tree.create_node(node_label, node_id, parent=parent_id)\n",
    "\n",
    "    # Save the tree for this combination\n",
    "    trees[(temperature, top_p)] = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "input_statements = pd.read_csv('DirectStatements.csv', header=None)[0].tolist()[7:]\n",
    "\n",
    "print(len(input_statements))\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_softened_statement(statement):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please help rephrase the following statement into a question that gently encourages a person to consider their own perspective on their habits. The rephrased question should be open-ended, imply no judgment, and convey understanding and empathy.\"},\n",
    "            {\"role\": \"user\", \"content\": statement}\n",
    "        ]\n",
    "    )\n",
    "    return(completion.choices[0].message)\n",
    "    \n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement in input_statements:\n",
    "    output = generate_softened_statement(statement)\n",
    "    meets_criteria = \"Manual Review Needed\"\n",
    "    results.append([statement, output, meets_criteria])\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file\n",
    "results_df = pd.DataFrame(results, columns=['Input Statement', 'Produced Output', 'Meets Criterion'])\n",
    "results_df.to_csv('A4_4_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "input_statements = pd.read_csv('DirectStatements.csv', header=None)[0].tolist()\n",
    "\n",
    "print(len(input_statements))\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_softened_statement(statement):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please help rephrase the following statement into a question that gently encourages a person to consider their own perspective on their habits. The rephrased question should be open-ended, imply no judgment, and convey understanding and empathy.\"},\n",
    "            {\"role\": \"user\", \"content\": statement}\n",
    "        ]\n",
    "    )\n",
    "    return(completion.choices[0].message.content)\n",
    "    \n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement in input_statements:\n",
    "    output = generate_softened_statement(statement)\n",
    "    meets_criteria = \"Manual Review Needed\"\n",
    "    results.append([statement, 0])\n",
    "    results.append([output, 1])\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('A4_5_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You dislike the fact that your health is slowly deteriorating after each cigarette.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "input_statements = pd.read_csv('A4_5_1.csv', header=None)[0].tolist()\n",
    "input_flag = pd.read_csv('A4_5_1.csv', header=None)[1].tolist()\n",
    "\n",
    "print(input_statements[0])\n",
    "print(input_flag[0])\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_softened_statement(statement):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please evaluate the following statements. A 'soft' statement should gently invite dialogue without making the listener defensive. It should: Avoid direct confrontation or judgment. Use language that opens a discussion, allowing the listener to share personal thoughts and feelings.Express empathy, showing understanding and care.Be tentative, indicating that the speaker is suggesting rather than asserting.Encourage the listener to independently reflect and draw conclusions.Acknowledge that the listener is the foremost expert on their own life.For each statement, indicate if it embodies these qualities of 'softness' by responding with 'Soft' or if it fails to meet the criteria, respond with 'Not Soft'.\"},\n",
    "            {\"role\": \"user\", \"content\": statement}\n",
    "        ]\n",
    "    )\n",
    "    return(completion.choices[0].message.content)\n",
    "    \n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(input_statements)):\n",
    "    output = generate_softened_statement(input_statements[i])\n",
    "    results.append([input_statements[i], input_flag[i], output])\n",
    "    if (input_flag[i] == 1 and output == \"Soft\") or (input_flag[i] == 0 and output == \"Not Soft\"):\n",
    "        count += 1\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file\n",
    "results_df = pd.DataFrame(results, columns=[\"statement\", \"label\", \"output\"])\n",
    "results_df.to_csv('A4_5_3.csv', index=False)\n",
    "print(count/len(input_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You dislike the fact that your health is slowly deteriorating after each cigarette.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "input_statements = pd.read_csv('A4_5_1.csv', header=None)[0].tolist()\n",
    "input_flag = pd.read_csv('A4_5_1.csv', header=None)[1].tolist()\n",
    "\n",
    "print(input_statements[0])\n",
    "print(input_flag[0])\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_softened_statement(statement):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"In the following task, I'll provide you with statements. For each, I want you to think aloud and explain step by step how you determine if the statement is 'Soft' or 'Not Soft' based on the criteria for softness. A 'Soft' statement should be non-confrontational, encourage self-exploration with invitational language, show empathy and understanding, be tentative in its assertions, promote the listener's autonomy, and respect the listener's expertise about their own life. After your explanation, conclude with a clear 'Soft' or 'Not Soft' judgment.\"},\n",
    "            {\"role\": \"user\", \"content\": statement}\n",
    "        ]\n",
    "    )\n",
    "    return(completion.choices[0].message.content)\n",
    "    \n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(input_statements)):\n",
    "    output = generate_softened_statement(input_statements[i])\n",
    "    results.append([input_statements[i], input_flag[i], output])\n",
    "    if \"Not Soft\" in output and input_flag[i] == 1:\n",
    "        count += 1\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file\n",
    "results_df = pd.DataFrame(results, columns=[\"statement\", \"label\", \"output\"])\n",
    "results_df.to_csv('A4_6_2.csv', index=False)\n",
    "print((len(input_statements) - count)/len(input_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Have you ever considered what your experience might be like if you tried detoxification? 1 To determine if this statement is 'Soft' or 'Not Soft', I will check it against the criteria for softness. The statement is non-confrontational, meaning it doesn't suggest any kind of conflict or criticism. It seems to encourage self-exploration by asking the listener to consider their own experience and what it might be like in a hypothetical situation. The use of an open question (\"Have you ever considered...\") can be interpreted as invitational language, allowing the listener to engage in the conversation at their own comfort level. There is no assertion being made, so it indeed is tentative, merely posing a situation for the listener to contemplate. The question promotes the listener's autonomy by letting them evaluate and decide on their own. Lastly, the phrase respects the listener's expertise by giving them the opportunity to analyze their own potential experiences based on their knowledge of their life situation. Based on these factors, the statement would be categorized as 'Soft'.\n",
      "Have you ever taken a moment to explore your feelings about the possibility of not being able to find another partner if you decide to end your current relationship? 1 To determine if a statement is 'Soft' or 'Not Soft' involves evaluating whether it meets the known criteria. \n",
      "\n",
      "The statement, \"Have you ever taken a moment to explore your feelings about the possibility of not being able to find another partner if you decide to end your current relationship?\", first does not seem to be confrontational. It brings up a concern indirectly but does not force the receiver to defend their stance or feel attacked. Therefore, it's non-confrontational.\n",
      "\n",
      "Secondly, it encourages exploration of self-feelings about a certain situation which is a component of invitational language, promoting the listener to think and respond rather than giving them instruction or information.\n",
      "\n",
      "Also, this sentence shows empathy and understanding. The speaker acknowledges that the listener may have fear or worries about ending the relationship, thereby understanding their emotional position.\n",
      "\n",
      "Equally, the statement is somewhat tentative. It discusses a 'possibility,' and a hypothetical situation, rather than making firm predictions or judgments about the future.\n",
      "\n",
      "The sentence encourages the listener's autonomy by inviting them to think about their feelings and make a decision based on those feelings. This way, it respects the listener's ability to have expertise about their life and their situation.\n",
      "\n",
      "Therefore, upon review, the statement meets all the criteria for softness. Hence, it is a 'Soft' statement.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "c = 0\n",
    "for result in results:\n",
    "    if \"Not Soft\" in result[2] and result[1] == 1:\n",
    "        print(result[0], result[1], result[2])\n",
    "        c += 1\n",
    "\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
