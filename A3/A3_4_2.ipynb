{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load GPT-2 model\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=5)\n",
    "\n",
    "# Cell 2: Training arguments (no change)\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "# Cell 3: Instantiate datasets (may need to adapt for GPT-2 tokenization)\n",
    "from SentimentDataset import SentimentDataset\n",
    "\n",
    "train_dataset = SentimentDataset(split=\"train\")  \n",
    "val_dataset = SentimentDataset(split=\"validation\")\n",
    "\n",
    "# Cell 4 and Cell 5: Metrics and compute_metrics function (no change)\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009db40de64b4d26a1719ae8a8a27d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.474, 'learning_rate': 4.9010175396919665e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3764, 'learning_rate': 4.8020350793839334e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3757, 'learning_rate': 4.7030526190759e-05, 'epoch': 0.18}\n",
      "{'loss': 0.345, 'learning_rate': 4.6040701587678666e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3394, 'learning_rate': 4.5050876984598335e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3392, 'learning_rate': 4.4061052381518e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3207, 'learning_rate': 4.307122777843766e-05, 'epoch': 0.42}\n",
      "{'loss': 0.307, 'learning_rate': 4.208140317535733e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3005, 'learning_rate': 4.109157857227699e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2921, 'learning_rate': 4.010175396919666e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2885, 'learning_rate': 3.9111929366116324e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2717, 'learning_rate': 3.812210476303599e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2665, 'learning_rate': 3.7132280159955656e-05, 'epoch': 0.77}\n",
      "{'loss': 0.271, 'learning_rate': 3.614245555687532e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2466, 'learning_rate': 3.515263095379499e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2758, 'learning_rate': 3.4162806350714657e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2456, 'learning_rate': 3.317298174763432e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1915, 'learning_rate': 3.218315714455399e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1976, 'learning_rate': 3.119333254147365e-05, 'epoch': 1.13}\n",
      "{'loss': 0.1895, 'learning_rate': 3.0203507938393317e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2047, 'learning_rate': 2.9213683335312986e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1916, 'learning_rate': 2.822385873223265e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1915, 'learning_rate': 2.7234034129152314e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1919, 'learning_rate': 2.6244209526071984e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2143, 'learning_rate': 2.5254384922991646e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1803, 'learning_rate': 2.4264560319911315e-05, 'epoch': 1.54}\n",
      "{'loss': 0.1845, 'learning_rate': 2.3274735716830978e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1861, 'learning_rate': 2.2284911113750644e-05, 'epoch': 1.66}\n",
      "{'loss': 0.198, 'learning_rate': 2.129508651067031e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1833, 'learning_rate': 2.0305261907589976e-05, 'epoch': 1.78}\n",
      "{'loss': 0.1881, 'learning_rate': 1.931543730450964e-05, 'epoch': 1.84}\n",
      "{'loss': 0.1755, 'learning_rate': 1.8325612701429307e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1804, 'learning_rate': 1.7335788098348973e-05, 'epoch': 1.96}\n",
      "{'loss': 0.1745, 'learning_rate': 1.634596349526864e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1132, 'learning_rate': 1.5356138892188305e-05, 'epoch': 2.08}\n",
      "{'loss': 0.1321, 'learning_rate': 1.4366314289107971e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1097, 'learning_rate': 1.3376489686027638e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1265, 'learning_rate': 1.2386665082947303e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1214, 'learning_rate': 1.1396840479866969e-05, 'epoch': 2.32}\n",
      "{'loss': 0.117, 'learning_rate': 1.0407015876786634e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1266, 'learning_rate': 9.417191273706299e-06, 'epoch': 2.43}\n",
      "{'loss': 0.1233, 'learning_rate': 8.427366670625965e-06, 'epoch': 2.49}\n",
      "{'loss': 0.1355, 'learning_rate': 7.437542067545632e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1231, 'learning_rate': 6.447717464465297e-06, 'epoch': 2.61}\n",
      "{'loss': 0.1075, 'learning_rate': 5.457892861384962e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1267, 'learning_rate': 4.468068258304629e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1261, 'learning_rate': 3.478243655224295e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1286, 'learning_rate': 2.4884190521439603e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1099, 'learning_rate': 1.4985944490636262e-06, 'epoch': 2.91}\n",
      "{'loss': 0.1243, 'learning_rate': 5.087698459832917e-07, 'epoch': 2.97}\n",
      "{'train_runtime': 8463.8209, 'train_samples_per_second': 23.872, 'train_steps_per_second': 2.984, 'train_loss': 0.20911685609664582, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8027de2097d42ac9c8cef7a4f6f36d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.908256880733945\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, DataCollatorWithPadding, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Validation accuracy: \", results[\"eval_accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
