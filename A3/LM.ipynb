{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Top-Level Code/Notebook\n",
    "### Training a language model base on Karpathy's minGPT codebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nimport nltk\\nnltk.download('punkt')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below is needed for using Google Colab, so un comment this if that is what you're using\n",
    "\"\"\" \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Colab\\\\ Notebooks/\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below is also needed for using Google Colab\n",
    "# BEFORE executing this, you must place the mingpt folder supplied in the assignment\n",
    "# your google drive, within the folder \"Colab Notebooks\"\n",
    "#\n",
    "# It mounts and changes into the folder that contains mingpt, which you must upload to google drive\n",
    "# So un-comment it if you've uploaded mingpt to your google drive, into the  \"Colab Notebooks\" folder\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "from pathlib import Path \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.bpe import BPETokenizer \n",
    "from mingpt.utils import set_seed \n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the dataset to train the Language Model (LM)\n",
    "This implementation splits the sentences and so doesn't create training \n",
    "examples that cross sentences.\n",
    "\n",
    "This code is set so that it uses one of two possible datasets, which were also used in Assignment 1: \n",
    "SmallSimpleCorpus.txt or LargerCorpus.txt\n",
    "\n",
    "Arguments:\n",
    "            ds_choice: str. \"small\" or \"large\". (i.e. selects which of the two datasets)\n",
    "            split: str. \"train\" or \"test\".\n",
    "            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.\n",
    "\"\"\" \n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds_choice=\"small\", split=\"train\", truncation=-1):\n",
    "        \n",
    "        base_path = \"./\"\n",
    "        fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\"}\n",
    "        self.ds_choice = ds_choice\n",
    "        self.truncation = truncation  # int. If -1, then\n",
    "        text = Path(base_path, fn[ds_choice]).read_text()\n",
    "        if ds_choice == \"large\":\n",
    "            # Remove the newline char in the middle of sentences\n",
    "            # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
    "            text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Train / test split\n",
    "        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "        if split == \"train\":\n",
    "            raw_data = train \n",
    "        else:\n",
    "            raw_data = val \n",
    "\n",
    "        # Tokenize\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.data = []  # List of 1-d pytorch tensor\n",
    "        for sent in raw_data:\n",
    "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
    "            if truncation >= 0:\n",
    "                self.data.append(tokenized[:truncation])\n",
    "            else:\n",
    "                self.data.append(tokenized)\n",
    "\n",
    "        # Count some items\n",
    "        self.max_sentence_length = np.max([len(d) for d in self.data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), \n",
    "        but actually, only a small number of vocab is used, especially for the small text. \n",
    "        \"\"\"\n",
    "        return 50257\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The output should be a tuple x and y, both as pytorch tensors.\n",
    "        Please refer to the `run()` method in the mingpt/trainer.py script for \n",
    "        how the x and y are going to be used.\n",
    "        \"\"\"\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return (x, y)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        \"\"\"\n",
    "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
    "        \"\"\"\n",
    "        return self.max_sentence_length\n",
    "    \n",
    "# Instantiate the Training Dataset\n",
    "# train_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"train\")  # use this for the short corpus\n",
    "train_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"train\", truncation=512) #use this for long\n",
    "\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "# val_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"validation\")\n",
    "val_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"validation\", truncation=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  # List (len B) of varying lengths\n",
    "    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n",
    "    maxlen = max([len(s) for s in x])\n",
    "\n",
    "    padded_x, padded_y = [], []\n",
    "    for sx, sy in zip(x, y):\n",
    "        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n",
    "        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n",
    "    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26788,   286, 11620,   290,  4898,   373,   287, 19133,   287,   262,\n",
      "         1903,  1528,   286, 10598,    26,   290,   262, 30962,   286,   311,\n",
      "         1789,    11, 28630,    11,   290,   617,  3354,   286,  5478,   973,\n",
      "          262, 37928,    12, 25717,  9875,   563,  7582,   284,  2380,  1988,\n",
      "           11,   290,   617, 23325, 33831,   326,   340,   318,   991,   287,\n",
      "          779,   287,   262,  6569, 16690,   286,   262,   938,    12, 13190,\n",
      "         1499]) tensor([  286, 11620,   290,  4898,   373,   287, 19133,   287,   262,  1903,\n",
      "         1528,   286, 10598,    26,   290,   262, 30962,   286,   311,  1789,\n",
      "           11, 28630,    11,   290,   617,  3354,   286,  5478,   973,   262,\n",
      "        37928,    12, 25717,  9875,   563,  7582,   284,  2380,  1988,    11,\n",
      "          290,   617, 23325, 33831,   326,   340,   318,   991,   287,   779,\n",
      "          287,   262,  6569, 16690,   286,   262,   938,    12, 13190,  1499,\n",
      "           13])\n",
      "X:  Money of leather and wood was in circulation in the early days of Rome; and the natives of Siam, Bengal, and some parts of Africa used the brilliantly-colored cowry shell to represent value, and some travelers allege that it is still in use in the remote portions of the last-named country\n",
      "Y:   of leather and wood was in circulation in the early days of Rome; and the natives of Siam, Bengal, and some parts of Africa used the brilliantly-colored cowry shell to represent value, and some travelers allege that it is still in use in the remote portions of the last-named country.\n"
     ]
    }
   ],
   "source": [
    "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
    "x,y = train_dataset[5]\n",
    "print(x, y)\n",
    "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
    "print(\"Y: \",train_dataset.tokenizer.decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.52M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a Trainer object and set the core hyper-parameters\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "# train_config.max_iters = 3000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
    "train_config.max_iters = 100000\n",
    "train_config.num_workers = 0\n",
    "# train_config.batch_size = 4    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "train_config.batch_size = 16    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=lm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 117.79ms; iter 0: train loss 0.94879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/qujianning/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/LM.ipynb 单元格 10\u001b[0m line \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/LM.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_callback(\u001b[39m'\u001b[39m\u001b[39mon_batch_end\u001b[39m\u001b[39m'\u001b[39m, batch_end_callback)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/LM.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Train!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/LM.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/mingpt/trainer.py:213\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m    212\u001b[0m model\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    214\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), config\u001b[39m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This function is called at the end of every batch in training\n",
    "# and is used to report the amount of time per 100 batches, and the loss at that point\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# Train!\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(trainer.device)\n",
    "# store the saved model in a file, so can re-use later\n",
    "modelsavename= \"model_filename.pt\"  # change the name here to save in a specific file (and restore below)\n",
    "with open(modelsavename, \"wb\") as f:\n",
    "    torch.save(trainer.model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He and I can hold a dog. dog. cat. cat'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the trained language model to predict a sequence of words following a few words\n",
    "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
    "generated_sequence, probs = trainer.model.modified_generate_1(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 0.3650, 0.5280, 0.5440, 0.5910, 0.9980, 0.4950,\n",
       "         0.9410, 0.5670, 0.5420, 0.7700]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(probs, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She rubs a dog. cat. cat. dog and dog'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
    "generated_sequence, probs = trainer.model.modified_generate_1(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 0.4050, 0.6510, 0.8100, 0.9640, 0.9990, 0.6730,\n",
       "         0.5090, 0.5240, 0.6770, 1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(probs, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rub a dog. cat.. cat and dog.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My example\n",
    "encoded_prompt = train_dataset.tokenizer(\"I rub\").to(trainer.device)\n",
    "generated_sequence, probs = trainer.model.modified_generate_1(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.5100, 0.5960, 0.5500, 0.8380, 0.9710, 0.9350, 0.7110,\n",
       "         0.5680, 0.9980, 0.9160]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(probs, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below shows how to reload the model from the saved file; is useful things that take long to train\n",
    "model.load_state_dict(torch.load(modelsavename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She rubs the dog and cat. cat. cat. dog'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example showing how the reloaded model still works\n",
    "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
    "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He and I can hold a dog. cat. cat and dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the trained language model to predict a sequence of words following a few words\n",
    "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
    "generated_sequence, idx, probs = trainer.model.modified_generate_2(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can  0.555</td>\n",
       "      <td>hold  0.679</td>\n",
       "      <td>a  0.531</td>\n",
       "      <td>dog  0.614</td>\n",
       "      <td>.  0.998</td>\n",
       "      <td>cat  0.667</td>\n",
       "      <td>.  0.986</td>\n",
       "      <td>cat  0.645</td>\n",
       "      <td>and  0.676</td>\n",
       "      <td>dog  0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hold  0.288</td>\n",
       "      <td>rub  0.319</td>\n",
       "      <td>the  0.465</td>\n",
       "      <td>cat  0.386</td>\n",
       "      <td>.  0.002</td>\n",
       "      <td>dog  0.331</td>\n",
       "      <td>and  0.012</td>\n",
       "      <td>dog  0.353</td>\n",
       "      <td>.  0.321</td>\n",
       "      <td>cat  0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rub  0.152</td>\n",
       "      <td>can  0.001</td>\n",
       "      <td>and  0.004</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>.  0.002</td>\n",
       "      <td>a  0.001</td>\n",
       "      <td>a  0.001</td>\n",
       "      <td>can  0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holds  0.003</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>hold  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>the  0.001</td>\n",
       "      <td>the  0.001</td>\n",
       "      <td>rub  0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>dog  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>and  0.001</td>\n",
       "      <td>can  0.000</td>\n",
       "      <td>holds  0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog  0.000</td>\n",
       "      <td>dog  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1              2            3            4  \\\n",
       "0     can  0.555   hold  0.679       a  0.531   dog  0.614     .  0.998   \n",
       "1    hold  0.288    rub  0.319     the  0.465   cat  0.386     .  0.002   \n",
       "2     rub  0.152    can  0.001     and  0.004     a  0.000   and  0.000   \n",
       "3   holds  0.003    the  0.000    hold  0.000   the  0.000   rub  0.000   \n",
       "4     and  0.000      a  0.000     cat  0.000   and  0.000   dog  0.000   \n",
       "5     dog  0.000    dog  0.000   holds  0.000   rub  0.000   cat  0.000   \n",
       "\n",
       "             5            6            7            8              9  \n",
       "0   cat  0.667     .  0.986   cat  0.645   and  0.676     dog  0.991  \n",
       "1   dog  0.331   and  0.012   dog  0.353     .  0.321     cat  0.005  \n",
       "2     a  0.000     .  0.002     a  0.001     a  0.001     can  0.001  \n",
       "3   the  0.000     a  0.000   the  0.001   the  0.001     rub  0.001  \n",
       "4   and  0.000   the  0.000   and  0.001   can  0.000   holds  0.001  \n",
       "5     .  0.000   cat  0.000     .  0.000     .  0.000     and  0.000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = []\n",
    "for i, x in enumerate(idx):\n",
    "    col = []\n",
    "    for j, y in enumerate(x):\n",
    "        col.append(train_dataset.tokenizer.decode(idx[i][j].reshape(1)) + f\"  {probs[i][j]:.3f}\")\n",
    "    word.append(col)\n",
    "    \n",
    "import pandas as pd\n",
    "result = pd.DataFrame(data=word)\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.decode(generated_sequence[0][0].reshape(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She rubs a cat and dog. cat. cat. dog'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
    "generated_sequence, idx, probs = trainer.model.modified_generate_2(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a  0.493</td>\n",
       "      <td>cat  0.569</td>\n",
       "      <td>and  0.679</td>\n",
       "      <td>dog  0.998</td>\n",
       "      <td>.  0.999</td>\n",
       "      <td>cat  0.508</td>\n",
       "      <td>.  0.998</td>\n",
       "      <td>cat  0.724</td>\n",
       "      <td>.  0.998</td>\n",
       "      <td>dog  0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the  0.387</td>\n",
       "      <td>dog  0.431</td>\n",
       "      <td>.  0.321</td>\n",
       "      <td>cat  0.002</td>\n",
       "      <td>.  0.001</td>\n",
       "      <td>dog  0.489</td>\n",
       "      <td>and  0.001</td>\n",
       "      <td>dog  0.275</td>\n",
       "      <td>.  0.001</td>\n",
       "      <td>cat  0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and  0.120</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>a  0.001</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>and  0.001</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>and  0.001</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>I  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>dog  0.000</td>\n",
       "      <td>the  0.001</td>\n",
       "      <td>can  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>can  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>hold  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1            2            3            4  \\\n",
       "0       a  0.493     cat  0.569   and  0.679   dog  0.998     .  0.999   \n",
       "1     the  0.387     dog  0.431     .  0.321   cat  0.002     .  0.001   \n",
       "2     and  0.120     and  0.000     a  0.000     a  0.000   and  0.000   \n",
       "3       .  0.000       .  0.000   the  0.000   the  0.000   cat  0.000   \n",
       "4   holds  0.000   holds  0.000     .  0.000   rub  0.000   dog  0.000   \n",
       "5     rub  0.000     rub  0.000   can  0.000   and  0.000   rub  0.000   \n",
       "\n",
       "             5              6            7             8              9  \n",
       "0   cat  0.508       .  0.998   cat  0.724      .  0.998     dog  0.776  \n",
       "1   dog  0.489     and  0.001   dog  0.275      .  0.001     cat  0.224  \n",
       "2     a  0.001       .  0.000   and  0.001    rub  0.000       a  0.000  \n",
       "3   and  0.001     rub  0.000     a  0.000      I  0.000     the  0.000  \n",
       "4   the  0.001     can  0.000   the  0.000    and  0.000     and  0.000  \n",
       "5     .  0.000   holds  0.000     .  0.000   hold  0.000   holds  0.000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = []\n",
    "for i, x in enumerate(idx):\n",
    "    col = []\n",
    "    for j, y in enumerate(x):\n",
    "        col.append(train_dataset.tokenizer.decode(idx[i][j].reshape(1)) + f\"  {probs[i][j]:.3f}\")\n",
    "    word.append(col)\n",
    "    \n",
    "import pandas as pd\n",
    "result = pd.DataFrame(data=word)\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.52M\n",
      "running on device cpu\n",
      "She flips in the coins is always room. were\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Create and config model\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)\n",
    "\n",
    "# Load state dict\n",
    "modelsavename= \"model_large100K.pt\"\n",
    "model.load_state_dict(torch.load(modelsavename))\n",
    "\n",
    "# Create a Trainer object and set the core hyper-parameters\n",
    "train_config = Trainer.get_default_config()\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=lm_collate_fn)\n",
    "\n",
    "# Test the results\n",
    "encoded_prompt = train_dataset.tokenizer(\"She flips\").to(trainer.device)\n",
    "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "\n",
    "result = train_dataset.tokenizer.decode(generated_sequence[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like that time he was passed by Mr.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Test the results\n",
    "encoded_prompt = train_dataset.tokenizer(\"I like\").to(trainer.device)\n",
    "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "\n",
    "result = train_dataset.tokenizer.decode(generated_sequence[0])\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c245645368b405f9e41f3dedb59d0df7c5d5feced548513488e8eb3fe8134cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
