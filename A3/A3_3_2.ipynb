{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda==False\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "from pathlib import Path \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.bpe import BPETokenizer \n",
    "from mingpt.utils import set_seed \n",
    "set_seed(1234)\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from utils import *\n",
    "import datasets\n",
    "from SentimentDataset import SentimentDataset\n",
    "from LanguageModelingDataset import LanguageModelingDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Training Dataset\n",
    "train_dataset = SentimentDataset(split=\"train\")  # use this for the short corpus\n",
    "\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "val_dataset = LanguageModelingDataset(split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 5562,   705,    82,  1290,  1165, 15444,   284, 17004,   884, 31194,\n",
      "         3513,   220]), 0)\n",
      "X:  that 's far too tragic to merit such superficial treatment \n",
      "Y:  0\n"
     ]
    }
   ],
   "source": [
    "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
    "x,y = train_dataset[5]\n",
    "print(train_dataset[5])\n",
    "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
    "print(\"Y: \",(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.52M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)\n",
    "# model.to(trainer.device)\n",
    "\n",
    "modelsavename= \"model_large100K.pt\"\n",
    "model.load_state_dict(torch.load(modelsavename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/qujianning/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/A3_3_2.ipynb 单元格 5\u001b[0m line \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/A3_3_2.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_callback(\u001b[39m'\u001b[39m\u001b[39mon_batch_end\u001b[39m\u001b[39m'\u001b[39m, sen_batch_end_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/A3_3_2.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Train!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/qujianning/Desktop/UT%E8%AF%BE%E7%A8%8B/2023%20Fall/ECE1786%20NLP/Assignments/Natrual_Language_Processing_Project/A3/A3_3_2.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/mingpt/trainer.py:143\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(val_iter)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     val_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(val_loader)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/mingpt/trainer.py:95\u001b[0m, in \u001b[0;36mTrainer.run.<locals>.<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownstream_finetune:\n\u001b[1;32m     78\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     79\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset,\n\u001b[1;32m     80\u001b[0m         sampler\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mRandomSampler(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset, replacement\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_samples\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m1e10\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m batch: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     val_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     89\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_dataset,\n\u001b[1;32m     90\u001b[0m         sampler\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mRandomSampler(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_dataset, replacement\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_samples\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m1e10\u001b[39m)),\n\u001b[1;32m     91\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m         pin_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m         batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m     94\u001b[0m         num_workers\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mnum_workers,\n\u001b[0;32m---> 95\u001b[0m         collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m batch: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/UT课程/2023 Fall/ECE1786 NLP/Assignments/Natrual_Language_Processing_Project/A3/utils.py:38\u001b[0m, in \u001b[0;36msen_collate_fn\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m     padded_x\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mcat([sx, torch\u001b[39m.\u001b[39mones(maxlen \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(sx))]))\n\u001b[1;32m     37\u001b[0m ret_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(padded_x)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m ret_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mas_tensor(y)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m ret_x, ret_y\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# Create a Trainer object and set the core hyper-parameters\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 3000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 8    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=sen_collate_fn, downstream_finetune=True)\n",
    "trainer.set_callback('on_batch_end', sen_batch_end_callback)\n",
    "# Train!\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
